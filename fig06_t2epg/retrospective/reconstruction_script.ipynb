{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cfl\n",
    "import bart \n",
    "import math\n",
    "\n",
    "from python_utils import models\n",
    "from python_utils import signalprocessing as sig\n",
    "from python_utils import simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_one_dim(x: torch.Tensor, shift: int, dim: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Similar to roll but for only one dim.\n",
    "    Args:\n",
    "        x: A PyTorch tensor.\n",
    "        shift: Amount to roll.\n",
    "        dim: Which dimension to roll.\n",
    "    Returns:\n",
    "        Rolled version of x.\n",
    "    \"\"\"\n",
    "    shift = shift % x.size(dim)\n",
    "    if shift == 0:\n",
    "        return x\n",
    "\n",
    "    left = x.narrow(dim, 0, x.size(dim) - shift)\n",
    "    right = x.narrow(dim, x.size(dim) - shift, shift)\n",
    "\n",
    "    return torch.cat((right, left), dim=dim)\n",
    "\n",
    "def roll(\n",
    "    x: torch.Tensor,\n",
    "    shift: List[int],\n",
    "    dim: List[int],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Similar to np.roll but applies to PyTorch Tensors.\n",
    "    Args:\n",
    "        x: A PyTorch tensor.\n",
    "        shift: Amount to roll.\n",
    "        dim: Which dimension to roll.\n",
    "    Returns:\n",
    "        Rolled version of x.\n",
    "    \"\"\"\n",
    "    if len(shift) != len(dim):\n",
    "        raise ValueError(\"len(shift) must match len(dim)\")\n",
    "\n",
    "    for (s, d) in zip(shift, dim):\n",
    "        x = roll_one_dim(x, s, d)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def fftshift(x: torch.Tensor, dim: Optional[List[int]] = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Similar to np.fft.fftshift but applies to PyTorch Tensors\n",
    "    Args:\n",
    "        x: A PyTorch tensor.\n",
    "        dim: Which dimension to fftshift.\n",
    "    Returns:\n",
    "        fftshifted version of x.\n",
    "    \"\"\"\n",
    "    if dim is None:\n",
    "        # this weird code is necessary for toch.jit.script typing\n",
    "        dim = [0] * (x.dim())\n",
    "        for i in range(1, x.dim()):\n",
    "            dim[i] = i\n",
    "\n",
    "    # also necessary for torch.jit.script\n",
    "    shift = [0] * len(dim)\n",
    "    for i, dim_num in enumerate(dim):\n",
    "        shift[i] = x.shape[dim_num] // 2\n",
    "\n",
    "    return roll(x, shift, dim)\n",
    "\n",
    "\n",
    "def ifftshift(x: torch.Tensor, dim: Optional[List[int]] = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Similar to np.fft.ifftshift but applies to PyTorch Tensors\n",
    "    Args:\n",
    "        x: A PyTorch tensor.\n",
    "        dim: Which dimension to ifftshift.\n",
    "    Returns:\n",
    "        ifftshifted version of x.\n",
    "    \"\"\"\n",
    "    if dim is None:\n",
    "        # this weird code is necessary for toch.jit.script typing\n",
    "        dim = [0] * (x.dim())\n",
    "        for i in range(1, x.dim()):\n",
    "            dim[i] = i\n",
    "\n",
    "    # also necessary for torch.jit.script\n",
    "    shift = [0] * len(dim)\n",
    "    for i, dim_num in enumerate(dim):\n",
    "        shift[i] = (x.shape[dim_num] + 1) // 2\n",
    "\n",
    "    return roll(x, shift, dim)\n",
    "\n",
    "tonpy = lambda x: torch.view_as_complex(x).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading dataset and setting model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_type     = 3\n",
    "latent_num     = 1\n",
    "epoch          = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask    = cfl.readcfl('data/mask')\n",
    "kspace  = cfl.readcfl('data/kspace')\n",
    "coils   = cfl.readcfl('data/coils')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining operators and preparing k-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[M,N,C,E] = kspace.shape\n",
    "\n",
    "coils_torch   = torch.stack((torch.tensor(np.real(coils),dtype=torch.float),\\\n",
    "                torch.tensor(np.imag(coils),dtype=torch.float)),dim=-1).to(device).reshape(M,N,C,1,2)\n",
    "coils_torch.requires_grad = False\n",
    "\n",
    "mask_torch    = torch.stack((torch.tensor(mask,dtype=torch.float),\\\n",
    "                             torch.tensor(mask,dtype=torch.float)),dim=-1).to(device)\n",
    "mask_torch.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfor(x,CC):\n",
    "    '''\n",
    "    Performs forward coil sensitivity multiplication\n",
    "    inputs:\n",
    "        x  (M x N x 1 x T x 2)    - time series of images\n",
    "        CC (M x N x C x 1 x 2)    - coil sensitivity functions\n",
    "    output\n",
    "        out (M x N x C x T x 2)   - output\n",
    "    '''\n",
    "    return torch.stack((x[..., 0] * CC[..., 0] - x[..., 1] * CC[..., 1],\\\n",
    "                       x[..., 0] * CC[..., 1] + x[..., 1] * CC[..., 0]),-1)\n",
    "\n",
    "def ffor(data):\n",
    "    '''\n",
    "    Performs a forward fourier transform\n",
    "    inputs:\n",
    "          x (M x N x C x T x 2)\n",
    "    outputs:\n",
    "        out (M x N x X x T x 2)\n",
    "             \n",
    "    '''\n",
    "    \n",
    "    return ifftshift(torch.fft(fftshift(data,dim = [0,1]).permute(2,3,0,1,4),signal_ndim = 2,normalized = True).permute(2,3,0,1,4),dim = [0,1])\n",
    "\n",
    "def R(data,mask):\n",
    "    '''\n",
    "    Apply undersampling mask to some input\n",
    "    inputs:\n",
    "        data  (M x N x C x T x 2)     - input to be masked\n",
    "        mask  (M x N x 1 x T x 1)     - mask to be applied\n",
    "    '''\n",
    "    \n",
    "    return data * mask\n",
    "\n",
    "def Dfor(model):\n",
    "    '''\n",
    "    Takes an input data through the decoder trained for compressing signal evolution\n",
    "    input\n",
    "        data (M*N x L x 2)   - input data\n",
    "        model              - neural network model\n",
    "    output\n",
    "        out (256 x 256 x 1 x T x 2)\n",
    "    '''\n",
    "    \n",
    "    out = model.decode(x)\n",
    "    return torch.stack((out * xr,out * xi),-1).reshape(M,N,1,E,2)\n",
    "\n",
    "kspace_torch   = torch.stack((torch.tensor(np.real(kspace),dtype=torch.float),\\\n",
    "                torch.tensor(np.imag(kspace),dtype=torch.float)),dim=-1).to(device)\n",
    "kspace_torch.requires_grad = False\n",
    "\n",
    "kspace_torch = R(kspace_torch,mask_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoder reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'data/model_type_%d_latent_%d_epochs_%d.pt' % (model_type,latent_num,epoch)\n",
    "model = torch.load(model_path).to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "lam         = 0\n",
    "iterations  = 2000\n",
    "lr          = 1e7\n",
    "\n",
    "saveiter    = 50\n",
    "dispiter    = 10\n",
    "\n",
    "x  = torch.zeros(M*N,latent_num).to(device).detach()\n",
    "\n",
    "xr = torch.zeros(M*N,1).to(device).detach() \n",
    "xi = torch.zeros(M*N,1).to(device).detach() \n",
    "\n",
    "\n",
    "x.requires_grad  = True\n",
    "xr.requires_grad = True\n",
    "xi.requires_grad = True\n",
    "\n",
    "criterion   = nn.MSELoss()\n",
    "\n",
    "all_T2   = []\n",
    "all_real = []\n",
    "all_imag = []\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "for iter in range(iterations):\n",
    "\n",
    "    loss   = criterion(kspace_torch,R(ffor(sfor(Dfor(model),coils_torch)),mask_torch))\n",
    "\n",
    "    gx = torch.autograd.grad(loss, \n",
    "                x, \n",
    "                create_graph = not True,retain_graph=True)[0]\n",
    "    gr = torch.autograd.grad(loss, \n",
    "                xr, \n",
    "                create_graph = not True,retain_graph=True)[0]\n",
    "    gi = torch.autograd.grad(loss, \n",
    "                xi, \n",
    "                create_graph = not True,retain_graph=True)[0]\n",
    "\n",
    "    x  = x  - gx * lr\n",
    "    xr = xr - gr * lr\n",
    "    xi = xi - gi * lr\n",
    "\n",
    "    running_loss = loss.item()\n",
    "\n",
    "    if iter % dispiter == 0:\n",
    "        toc = time.perf_counter()\n",
    "        print('iteration %d / %d, current loss: %.12f / elapsed time: %.2f (s)' % (iter,iterations,running_loss, toc-tic))\n",
    "        tic = time.perf_counter()\n",
    "        \n",
    "    if iter % saveiter == 0:\n",
    "        all_T2.append(np.squeeze(x.reshape(M,N,latent_num).detach()).cpu().numpy())\n",
    "        all_real.append(np.squeeze(xr.reshape(M,N,1).detach()).cpu().numpy())\n",
    "        all_imag.append(np.squeeze(xi.reshape(M,N,1).detach()).cpu().numpy())\n",
    "\n",
    "T2    = np.squeeze(x.reshape(M,N,latent_num).detach()).cpu().numpy()\n",
    "real  = np.squeeze(xr.reshape(M,N,1).detach()).cpu().numpy()\n",
    "imag  = np.squeeze(xi.reshape(M,N,1).detach()).cpu().numpy()\n",
    "\n",
    "timeseries_decoder = tonpy((Dfor(model).detach().squeeze().contiguous()))\n",
    "\n",
    "del x, xr, xi, gx, gr, gi, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = np.concatenate((np.expand_dims(T2,axis=-1),\\\n",
    "                          np.expand_dims(real,axis=-1),\\\n",
    "                          np.expand_dims(imag,axis=-1)),axis=-1).transpose(2,0,1)\n",
    "sig.mosaic(sig.nor(display),1,2 + latent_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimating t2 maps with dictionary matching in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "#preparing objectos to EPG simulate dictionary\n",
    "spacing     = 11.5 / 1000;\n",
    "angles_rad  = (torch.ones(1,E) * 180 * math.pi / 180).to(device)\n",
    "angle_exc   = (torch.ones(1,1) * 90 * math.pi/180).to(device)\n",
    "t2_range    = torch.linspace(0,400,1000).to(device) / 1000\n",
    "t1_range    = torch.ones(1000).to(device)*1000 / 1000\n",
    "\n",
    "angles_rad.requires_grad = False\n",
    "angle_exc.requires_grad  = False\n",
    "t1_range.requires_grad   = False\n",
    "t2_range.requires_grad   = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "# simulating the dictionary and encoding it as latent variables\n",
    "dictionary_epg        = simulator.FSE_signal2_ex(angle_exc,angles_rad,spacing,t1_range,t2_range)[0].squeeze()\n",
    "dictionary_epg_latent = model.encode(dictionary_epg).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tmap_est = []\n",
    "\n",
    "for ii in range(len(all_T2)):   \n",
    "    # performing vectorized dictionary \n",
    "    min_indices = torch.argmin(torch.abs(dictionary_epg_latent.permute((1,0)) - T2.reshape((M*N,1))),1)\n",
    "\n",
    "    # estimating t2 map\n",
    "    t2map_est = t2_range[min_indices].reshape(M,N).cpu().numpy()\n",
    "    \n",
    "    all_tmap_est.append(t2map_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computing EPG gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_norms_x  = np.zeros(len(all_tmap_est))\n",
    "all_norms_xr = np.zeros(len(all_tmap_est))\n",
    "\n",
    "for ii in range(len(all_tmap_est)):\n",
    "    print('gradient %d/%d' % (ii+1,len(all_tmap_est)))\n",
    "    x_epg   = torch.tensor(all_tmap_est[ii],dtype = torch.float).reshape(M*N).to(device).detach()\n",
    "    xr_epg  = torch.tensor(all_real[ii],dtype = torch.float).reshape(M*N,1,1).to(device).detach()\n",
    "\n",
    "    xi_epg  = torch.tensor(all_imag[ii],dtype = torch.float).reshape(M*N,1,1).to(device).detach()\n",
    "    x_t1    = torch.ones(M*N).to(device)\n",
    "\n",
    "    x_epg.requires_grad  = True\n",
    "    xr_epg.requires_grad = True\n",
    "    xi_epg.requires_grad = False\n",
    "    x_t1.requires_grad   = False\n",
    "\n",
    "    out    = simulator.FSE_signal2_ex(angle_exc,angles_rad,spacing,x_t1,x_epg)[0]\n",
    "    out    = torch.stack((out * xr_epg,out * xi_epg),-1).reshape(M,N,1,E,2)\n",
    "    loss   = criterion(kspace_torch,R(ffor(sfor(out,coils_torch)),mask_torch))\n",
    "\n",
    "    g = torch.autograd.grad(loss, \n",
    "                    x_epg, \n",
    "                    create_graph = not True,retain_graph=True)[0]\n",
    "    gp = torch.autograd.grad(loss, \n",
    "                xr_epg, \n",
    "                create_graph = not True,retain_graph=False)[0]\n",
    "    \n",
    "    all_norms_x[ii]  = torch.norm(g)\n",
    "    all_norms_xr[ii] = torch.norm(gp)\n",
    "    \n",
    "    del x_epg,xr_epg,xi_epg,x_t1,out,loss,g,gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting the norm of the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nor = np.max(np.concatenate((all_norms_x,all_norms_xr)))\n",
    "lw  = 3\n",
    "\n",
    "plt.title('norm of gradients in EPG Forward Model',fontsize = 24)\n",
    "plt.plot(all_norms_x / nor,linewidth=lw)\n",
    "plt.plot(all_norms_xr /nor,linewidth=lw)\n",
    "plt.legend(['T2 gradient','density gradient'],prop={'size':20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EPG reconstruction initialized with decoder reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 200\n",
    "lr         = 1e7\n",
    "\n",
    "x_epg   = torch.tensor(all_tmap_est[-1],dtype = torch.float).reshape(M*N).to(device).detach()\n",
    "xr_epg  = torch.tensor(all_real[-1],dtype = torch.float).reshape(M*N,1,1).to(device).detach()\n",
    "xi_epg  = torch.tensor(all_imag[-1],dtype = torch.float).reshape(M*N,1,1).to(device).detach()\n",
    "x_t1    = torch.ones(M*N).to(device)\n",
    "\n",
    "x_epg.requires_grad  = True\n",
    "xr_epg.requires_grad = True\n",
    "xi_epg.requires_grad = False\n",
    "x_t1.requires_grad   = False\n",
    "\n",
    "tic = time.perf_counter()\n",
    "for iter in range(iterations):\n",
    "    out    = simulator.FSE_signal2_ex(angle_exc,angles_rad,spacing,x_t1,x_epg)[0]\n",
    "    out    = torch.stack((out * xr_epg,out * xi_epg),-1).reshape(M,N,1,E,2)\n",
    "    loss   = criterion(kspace_torch,R(ffor(sfor(out,coils_torch)),mask_torch))\n",
    "\n",
    "    g = torch.autograd.grad(loss, \n",
    "                    x_epg, \n",
    "                    create_graph = not True,retain_graph=True)[0]\n",
    "    gp = torch.autograd.grad(loss, \n",
    "                xr_epg, \n",
    "                create_graph = not True,retain_graph=False)[0]\n",
    "\n",
    "    x_epg  = x_epg  - lr * g\n",
    "    xr_epg = xr_epg - lr * gp\n",
    "\n",
    "    running_loss = loss.item()\n",
    "    \n",
    "    if iter % dispiter == 0:\n",
    "        toc = time.perf_counter()\n",
    "        print('iteration %d / %d, current loss: %.12f / elapsed: %.2f' % (iter,iterations,running_loss,toc-tic))\n",
    "        tic = time.perf_counter()\n",
    "\n",
    "    del out,loss,g,gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2_epg    = np.squeeze(x_epg.reshape(M,N,latent_num).detach()).cpu().numpy()\n",
    "real_epg  = np.squeeze(xr_epg.reshape(M,N,1).detach()).cpu().numpy()\n",
    "imag_epg  = np.squeeze(xi_epg.reshape(M,N,1).detach()).cpu().numpy()\n",
    "\n",
    "out    = simulator.FSE_signal2_ex(angle_exc.detach().cpu(),angles_rad.detach().cpu(),spacing,x_t1.detach().cpu(),x_epg.detach().cpu())[0]\n",
    "out    = torch.stack((out * xr_epg.detach().cpu(),out * xi_epg.detach().cpu()),-1).reshape(M,N,1,E,2)\n",
    "\n",
    "timeseries_epg = tonpy(out.detach().squeeze().contiguous())\n",
    "\n",
    "del out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparing decoder and epg initialized decoder reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_truth = np.abs(cfl.readcfl('data/timeseries_truth'))\n",
    "\n",
    "#Masking out results outside of the brain\n",
    "brain_mask = np.zeros((M,N))\n",
    "brain_mask[np.where(np.abs(timeseries_truth[:,:,0])>0)] =1\n",
    "brain_mask = np.expand_dims(brain_mask,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_idx     = 0\n",
    "regidx_bart = 0 \n",
    "\n",
    "ech          = 0\n",
    "\n",
    "truth_show   = sig.nor(timeseries_truth[:,:,ech:ech+1]*brain_mask)\n",
    "decoder_show = sig.nor(np.abs(timeseries_decoder[:,:,ech:ech+1])*brain_mask)\n",
    "epg_show     = sig.nor(np.abs(timeseries_epg[:,:,ech:ech+1])*brain_mask) \n",
    "\n",
    "display = np.concatenate((decoder_show,epg_show),axis = -1).transpose(2,0,1)\n",
    "\n",
    "sig.mosaic(sig.nor(display),1,2,clim=[0,1])\n",
    "\n",
    "print('  proposed:  %.2f' % (sig.rmse(truth_show,decoder_show)*100))\n",
    "print('  epg:       %.2f' % (sig.rmse(truth_show,epg_show)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
