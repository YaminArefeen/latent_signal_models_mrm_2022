{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from python_utils import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layers   = 2\n",
    "nonlinearity   = 'tanh'\n",
    "latent_nums    = [1,2,3,4]\n",
    "learning_rates = [1e-5,1e-5,1e-5,1e-5]\n",
    "all_epochs     = [200000,200000,200000,200000]\n",
    "\n",
    "test_skip_idx = 5   #grab every 5th entry of dictionary for testing\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading dictionary and setting training / testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the baseline dictionary\n",
    "dictionary = np.real(sio.loadmat('data/dictionary.mat')['dictionary'])\n",
    "t2_range   = sio.loadmat('data/dict_params.mat')['dictionary_params'][0][0][3]\n",
    "    \n",
    "    \n",
    "t2_range_training = np.expand_dims(np.delete(t2_range,slice(None,None,test_skip_idx)),axis=0)\n",
    "t2_range_testing  = t2_range[:,::test_skip_idx]\n",
    "\n",
    "dictionary_training = np.delete(dictionary,slice(None,None,test_skip_idx),axis=1)\n",
    "dictionary_testing  = dictionary[:,::test_skip_idx]\n",
    "    \n",
    "[T,Ntraining] = dictionary_training.shape\n",
    "[_,Ntesting]  = dictionary_testing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_dimension,model_layers = 2, nonlinearity = 'leakyrelu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        if(nonlinearity == 'leakyrelu'):\n",
    "            self.activation = F.leaky_relu\n",
    "        elif(nonlinearity == 'relu'):\n",
    "            self.activation = F.relu\n",
    "        elif(nonlinearity == 'tanh'):\n",
    "            self.activation = torch.tanh\n",
    "        elif(nonlinearity == 'nalini'):\n",
    "            self.activation = self.nalini\n",
    "        elif(nonlinearity == 'htanh'):\n",
    "            self.activation = F.hardtanh\n",
    "        elif(nonlinearity == 'selu'):\n",
    "            self.activation = torch.nn.SELU()\n",
    "            \n",
    "        self.encoding_layers = nn.ModuleList([])\n",
    "        for ii in range(model_layers):\n",
    "            if(ii < (model_layers - 1)):#last layer goes from current dimension directly to latent dimension\n",
    "                self.encoding_layers.append(nn.Linear(input_dimension // 2**ii, input_dimension // 2**(ii+1)))              \n",
    "            else:\n",
    "                self.encoding_layers.append(nn.Linear(input_dimension // 2**ii, hidden_dimension))\n",
    "                \n",
    "        \n",
    "        self.decoding_layers = nn.ModuleList([])\n",
    "        self.decoding_layers.append(nn.Linear(hidden_dimension,input_dimension//2**(model_layers-1)))\n",
    "        for ii in range(model_layers-1):\n",
    "            self.decoding_layers.append(nn.Linear(input_dimension//2**(model_layers-(ii+1)),\\\n",
    "                                       input_dimension//2**(model_layers-(ii+1)-1)))\n",
    "\n",
    "    def nalini(self,x):\n",
    "        return x + F.relu((x-1)/2) + F.relu((-x-1)/2)\n",
    "        \n",
    "    def decode(self,x):\n",
    "        out = x\n",
    "        out = self.decoding_layers[0](out)\n",
    "        for ii in range(len(self.decoding_layers)-1):\n",
    "            out = self.activation(out)\n",
    "            out = self.decoding_layers[ii+1](out)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def encode(self,x):\n",
    "        out = x\n",
    "        for layer in self.encoding_layers:\n",
    "            out = layer(out)\n",
    "            out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.decode(self.encode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "#re-shaping dictionary, casting it to right type, and sending it to GPU\n",
    "dictionary_for_training = torch.tensor(np.transpose(dictionary_training,(1,0)),dtype = torch.float).to(device)\n",
    "\n",
    "all_models = []\n",
    "\n",
    "for latent_num,epochs,learning_rate in zip(latent_nums,all_epochs,learning_rates):\n",
    "    starting_time = time.time()\n",
    "    \n",
    "    print('Latent Variables: %d || epochs: %d || learning rate: %.3f' % (latent_num,epochs,learning_rate))\n",
    "    \n",
    "    model = autoencoder(T,latent_num,model_layers,nonlinearity)\n",
    "        \n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(dictionary_for_training)\n",
    "        loss   = criterion(output,dictionary_for_training)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss = loss.item()\n",
    "        \n",
    "        if(epoch % 1000 == 0):\n",
    "            print('  iteration %d/%d current loss: %.12f' % (epoch,epochs,running_loss))\n",
    "    \n",
    "    print('  final loss: %.12f' % running_loss)\n",
    "    ending_time = time.time()\n",
    "    print('  elapsed time: %.2f min' % ((ending_time - starting_time)/60))\n",
    "\n",
    "    all_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computing training and testing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = lambda truth, comp: np.sqrt(np.sum((comp - truth)**2,keepdims=False,axis=0)) / np.sqrt(np.sum(truth**2,keepdims=False,axis=0))\n",
    "\n",
    "[u,s,v] = np.linalg.svd(dictionary,full_matrices=False)\n",
    "\n",
    "all_basis = []\n",
    "\n",
    "linear_errors_training   = []\n",
    "proposed_errors_training = []\n",
    "\n",
    "linear_errors_testing   = []\n",
    "proposed_errors_testing = []\n",
    "\n",
    "dictionary_linear_training   = []\n",
    "dictionary_proposed_training = []\n",
    "\n",
    "dictionary_linear_testing   = []\n",
    "dictionary_proposed_testing = []\n",
    "\n",
    "for latent_num, model in zip(latent_nums,all_models):\n",
    "    #determine basis\n",
    "    basis = u[:,0:latent_num]\n",
    "    \n",
    "    #estimate dictionaries with svd and autoencoder\n",
    "    dictionary_estimate_linear_training   = basis @ np.conj(basis.T) @ dictionary_training\n",
    "    dictionary_estimate_proposed_training = model(dictionary_for_training).cpu().detach().numpy().transpose((1,0))\n",
    "    \n",
    "    dictionary_estimate_linear_testing    = basis @ np.conj(basis.T) @ dictionary_testing\n",
    "    dictionary_for_testing                = \\\n",
    "        torch.tensor(np.transpose(dictionary_testing,(1,0)),dtype = torch.float).to(device)\n",
    "    dictionary_estimate_proposed_testing = \\\n",
    "        model(dictionary_for_testing).cpu().detach().numpy().transpose((1,0))\n",
    "\n",
    "    error_linear_training   = rmse(dictionary_training,dictionary_estimate_linear_training)\n",
    "    error_proposed_training = rmse(dictionary_training,dictionary_estimate_proposed_training)\n",
    "    \n",
    "    error_linear_testing    = rmse(dictionary_testing,dictionary_estimate_linear_testing)\n",
    "    error_proposed_testing  = rmse(dictionary_testing,dictionary_estimate_proposed_testing)\n",
    "    \n",
    "    linear_errors_training.append(error_linear_training)\n",
    "    proposed_errors_training.append(error_proposed_training)\n",
    "    \n",
    "    linear_errors_testing.append(error_linear_testing)\n",
    "    proposed_errors_testing.append(error_proposed_testing)\n",
    "    \n",
    "    dictionary_linear_training.append(dictionary_estimate_linear_training)\n",
    "    dictionary_proposed_training.append(dictionary_estimate_proposed_training)\n",
    "    \n",
    "    dictionary_linear_testing.append(dictionary_estimate_linear_testing)\n",
    "    dictionary_proposed_testing.append(dictionary_estimate_proposed_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entry by entry training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 3\n",
    "for latent_num,error_linear,error_proposed in zip(latent_nums,linear_errors_training,proposed_errors_training):\n",
    "    print('average percent errors:')\n",
    "    print('  proposed: %.2f' % (np.mean(error_proposed)*100))\n",
    "    print('  linear:   %.2f' % (np.mean(error_linear)*100))\n",
    "    \n",
    "    plt.title('Latent Variables: %d' % latent_num,fontsize=24)\n",
    "    plt.plot(error_proposed,linewidth=lw)\n",
    "    plt.plot(error_linear,linewidth=lw)\n",
    "    plt.legend(['proposed','linear'],prop={'size':20})\n",
    "    plt.ylim([0,.3])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entry by entry testing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 3\n",
    "for latent_num,error_linear,error_proposed in zip(latent_nums,linear_errors_testing,proposed_errors_testing):\n",
    "    print('average percent errors:')\n",
    "    print('  proposed: %.2f' % (np.mean(error_proposed)*100))\n",
    "    print('  linear:   %.2f' % (np.mean(error_linear)*100))\\\n",
    "    \n",
    "    plt.title('Latent Variables: %d' % latent_num,fontsize=24)\n",
    "    plt.plot(error_proposed,linewidth = lw)\n",
    "    plt.plot(error_linear,  linewidth = lw)\n",
    "    plt.legend(['proposed','linear'],prop={'size':20})\n",
    "    plt.ylim([0,.4])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
